{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Scraping Movie Data\n",
        "\n",
        "In this notebook, we'll walk through the process of scraping movie data from \"The Numbers\" website, covering films released between 2000 and 2025. We'll use various Python libraries to handle challenges like anti-bot protections, extract structured data from HTML, and process everything efficiently using concurrency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Our Scraping Environment\n",
        "\n",
        "First we install `cloudscraper`, a library that helps bypass Cloudflare anti-bot pages. This is crucial since many modern websites employ protection mechanisms that detect and block automated scraping attempts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kti2XsMbVL1",
        "outputId": "4829fe17-8460-4a3d-d210-249e83f34c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloudscraper in /usr/local/lib/python3.11/dist-packages (1.2.71)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (3.2.3)\n",
            "Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from cloudscraper) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.9.2->cloudscraper) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install cloudscraper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Required Libraries\n",
        "\n",
        "Then we import standard library modules for date handling and concurrency, along with third-party modules like BeautifulSoup, pandas, `cloudscraper`, and others used throughout the notebook. Each of these libraries serves a specific purpose:\n",
        "- BeautifulSoup helps us parse HTML content\n",
        "- cloudscraper bypasses anti-bot protection\n",
        "- pandas will organize our data\n",
        "- ThreadPoolExecutor enables parallel processing\n",
        "- tqdm provides progress bars to visualize our scraping progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "from datetime import datetime, timedelta\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Third-party imports\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "import pandas as pd\n",
        "from requests.adapters import HTTPAdapter\n",
        "from tqdm import tqdm\n",
        "from urllib3.util.retry import Retry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Robust Scraper Session\n",
        "\n",
        "This function creates and returns a `cloudscraper` session with specified retry logic, enabling repeated attempts for specific HTTP errors. We configure it with sensible defaults:\n",
        "- 5 total retries for failed requests\n",
        "- Exponential backoff to avoid overwhelming the server\n",
        "- Retry on common error codes (403, 429, 500, etc.)\n",
        "- A realistic user agent to appear more like a normal browser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cloudscraper_session(\n",
        "    total_retries=5,\n",
        "    backoff_factor=1,\n",
        "    status_forcelist=(403, 429, 500, 502, 503, 504),\n",
        "    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a CloudScraper session configured to retry on specific HTTP errors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a CloudScraper session (handles Cloudflare anti-bot challenges).\n",
        "    scraper = cloudscraper.create_scraper()\n",
        "\n",
        "    # Optionally update the User-Agent header.\n",
        "    scraper.headers.update({\"User-Agent\": user_agent})\n",
        "\n",
        "    # Configure the Retry strategy.\n",
        "    retries = Retry(\n",
        "        total=total_retries,\n",
        "        backoff_factor=backoff_factor,\n",
        "        status_forcelist=status_forcelist,\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retries)\n",
        "\n",
        "    # Mount the adapter to handle both HTTP and HTTPS.\n",
        "    scraper.mount(\"http://\", adapter)\n",
        "    scraper.mount(\"https://\", adapter)\n",
        "\n",
        "    return scraper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetching and Parsing Web Pages\n",
        "\n",
        "This function returns a BeautifulSoup object for a given URL, raising an HTTPError if the status code indicates a failure. It's a simple wrapper that handles the connection setup and HTML parsing in one convenient call, making our code more readable and maintainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ox3EcDsbPeT"
      },
      "outputs": [],
      "source": [
        "def extract_movies(url, session=None):\n",
        "    \"\"\"\n",
        "    Fetches and returns a BeautifulSoup object for the given URL,\n",
        "    using a CloudScraper session configured with retries.\n",
        "    \"\"\"\n",
        "    if session is None:\n",
        "        # Fallback to a default session if none provided\n",
        "        session = get_cloudscraper_session()\n",
        "\n",
        "    response = session.get(url)\n",
        "    # Raises an HTTPError if the status is 4xx or 5xx.\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return BeautifulSoup(response.text, \"html.parser\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Movie Data from Pages\n",
        "\n",
        "Now we use this function to find rows in the Numbers table, extract movie titles with their corresponding links, and store them in a dictionary. This step essentially maps each movie title to its detail page URL, allowing us to navigate the website programmatically and gather detailed information about each film."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCVbIgcpyD2c"
      },
      "outputs": [],
      "source": [
        "def extract_movies_with_links(soup):\n",
        "    movie_dict = {}\n",
        "\n",
        "    # Loop through each row in the table (excluding the header)\n",
        "    rows = soup.find_all(\"tr\")[1:]  # Skip the header row\n",
        "\n",
        "    for row in rows:\n",
        "        # Find the column that contains the movie name (it is in <b><a> tag)\n",
        "        movie_cell = row.find(\"td\", {\"class\": \"data\"})\n",
        "        if movie_cell:\n",
        "            movie_name_tag = movie_cell.find_next(\n",
        "                \"a\"\n",
        "            )  # Find the next <a> tag inside the cell\n",
        "            if movie_name_tag:\n",
        "                movie_name = movie_name_tag.text.strip()  # Get the movie name\n",
        "                movie_link = movie_name_tag[\"href\"]  # Get the href link\n",
        "                movie_dict[movie_name] = movie_link  # Add to dictionary\n",
        "\n",
        "    return movie_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Detailed Movie Information\n",
        "\n",
        "This function extracts additional information from a mobile layout page, such as synopsis, budget, running time, cast/crew details, and more. We've designed it to handle various data formats and structures on the page, extracting information from:\n",
        "- Synopsis sections\n",
        "- Financial metrics tables\n",
        "- Movie details tables\n",
        "- Cast listings\n",
        "- Production and technical credits\n",
        "\n",
        "The function organizes all this information into a structured dictionary that we'll later incorporate into our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as7w-0KI1XKy"
      },
      "outputs": [],
      "source": [
        "def extract_movie_info_mobile(soup):\n",
        "    \"\"\"\n",
        "    Extracts the desired movie information from a mobile-layout page.\n",
        "\n",
        "    Returns a dictionary with the following keys:\n",
        "      - synopsis: the movie synopsis text.\n",
        "      - opening_weekend: monetary value for opening weekend.\n",
        "      - percent_of_total_gross: the percentage (e.g. \"35.4%\") from the same cell.\n",
        "      - production_budget: text as appears in the metrics.\n",
        "      - domestic_release_date: the first date from the Domestic Releases row.\n",
        "      - running_time: running time (e.g. \"98 minutes\").\n",
        "      - keywords: a comma-separated string of keywords.\n",
        "      - source: the source text.\n",
        "      - genre: genre text.\n",
        "      - production_method: production method text.\n",
        "      - creative_type: creative type text.\n",
        "      - production_financing_companies: financing companies text.\n",
        "      - production_countries: production countries.\n",
        "      - languages: the languages.\n",
        "      - lead_ensemble_members: list of dicts with \"actor\" and \"role\"\n",
        "      - production_technical_credits: list of strings for credit rows (stops when an <hr> is encountered)\n",
        "\n",
        "    Args:\n",
        "        soup (BeautifulSoup): Parsed HTML of the movie page (mobile layout).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the extracted movie information.\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "\n",
        "    summary = soup.find(\"div\", id=\"summary_mobile\")\n",
        "    if summary:\n",
        "        # Extract Synopsis: find the <h2> with \"Synopsis\" and its next <p>.\n",
        "        syn_h2 = summary.find(\"h2\", string=lambda s: s and \"Synopsis\" in s)\n",
        "        if syn_h2:\n",
        "            syn_p = syn_h2.find_next_sibling(\"p\")\n",
        "            # In case the <p> includes extra content (like \"Metrics\"), split at \"Metrics\"\n",
        "            result[\"synopsis\"] = (\n",
        "                syn_p.get_text(strip=True).split(\"Metrics\")[0] if syn_p else \"\"\n",
        "            )\n",
        "        else:\n",
        "            result[\"synopsis\"] = \"\"\n",
        "\n",
        "        # Extract Metrics.\n",
        "        # Skip the table with id \"movie_ratings\" if present.\n",
        "        metrics_table = summary.find(\"table\", id=lambda i: i != \"movie_ratings\")\n",
        "        result[\"opening_weekend\"] = \"\"\n",
        "        result[\"percent_of_total_gross\"] = \"\"\n",
        "        result[\"production_budget\"] = \"\"\n",
        "        if metrics_table:\n",
        "            for row in metrics_table.find_all(\"tr\"):\n",
        "                cells = row.find_all(\"td\")\n",
        "                if len(cells) >= 2:\n",
        "                    label = cells[0].get_text(strip=True).replace(\"\\xa0\", \" \")\n",
        "                    value = cells[1].get_text(\" \", strip=True)\n",
        "                    if \"Opening Weekend:\" in label:\n",
        "                        if \"(\" in value and \")\" in value:\n",
        "                            open_val, paren_part = value.split(\"(\", 1)\n",
        "                            percent = paren_part.split(\")\")[0].split()[0]\n",
        "                            result[\"opening_weekend\"] = open_val.strip()\n",
        "                            result[\"percent_of_total_gross\"] = percent.strip()\n",
        "                        else:\n",
        "                            result[\"opening_weekend\"] = value.strip()\n",
        "                    elif \"Production Budget:\" in label:\n",
        "                        result[\"production_budget\"] = value.strip()\n",
        "        else:\n",
        "            result.setdefault(\"opening_weekend\", \"\")\n",
        "            result.setdefault(\"percent_of_total_gross\", \"\")\n",
        "            result.setdefault(\"production_budget\", \"\")\n",
        "\n",
        "        # Extract Movie Details.\n",
        "        details_h2 = summary.find(\"h2\", string=lambda s: s and \"Movie Details\" in s)\n",
        "        # Initialize keys with empty values.\n",
        "        for field in [\n",
        "            \"domestic_release_date\",\n",
        "            \"running_time\",\n",
        "            \"keywords\",\n",
        "            \"source\",\n",
        "            \"genre\",\n",
        "            \"production_method\",\n",
        "            \"creative_type\",\n",
        "            \"production_financing_companies\",\n",
        "            \"production_countries\",\n",
        "            \"languages\",\n",
        "        ]:\n",
        "            result[field] = \"\"\n",
        "        if details_h2:\n",
        "            details_table = details_h2.find_next(\"table\")\n",
        "            if details_table:\n",
        "                for row in details_table.find_all(\"tr\"):\n",
        "                    cells = row.find_all(\"td\")\n",
        "                    if len(cells) >= 2:\n",
        "                        lab = cells[0].get_text(strip=True).replace(\"\\xa0\", \" \")\n",
        "                        val = cells[1].get_text(\" \", strip=True)\n",
        "                        if lab.startswith(\"Domestic Releases:\"):\n",
        "                            # Extract the first date before any parenthesis.\n",
        "                            result[\"domestic_release_date\"] = val.split(\"(\")[0].strip()\n",
        "                        elif lab.startswith(\"Running Time:\"):\n",
        "                            result[\"running_time\"] = val\n",
        "                        elif lab.startswith(\"Keywords:\"):\n",
        "                            result[\"keywords\"] = val\n",
        "                        elif lab.startswith(\"Source:\"):\n",
        "                            result[\"source\"] = val\n",
        "                        elif lab.startswith(\"Genre:\"):\n",
        "                            result[\"genre\"] = val\n",
        "                        elif lab.startswith(\"Production Method:\"):\n",
        "                            result[\"production_method\"] = val\n",
        "                        elif lab.startswith(\"Creative Type:\"):\n",
        "                            result[\"creative_type\"] = val\n",
        "                        elif lab.startswith(\"Production/Financing Companies:\"):\n",
        "                            result[\"production_financing_companies\"] = val\n",
        "                        elif lab.startswith(\"Production Countries:\"):\n",
        "                            result[\"production_countries\"] = val\n",
        "                        elif lab.startswith(\"Languages:\"):\n",
        "                            result[\"languages\"] = val\n",
        "    else:\n",
        "        # If no summary container is found.\n",
        "        for key in [\n",
        "            \"synopsis\",\n",
        "            \"opening_weekend\",\n",
        "            \"percent_of_total_gross\",\n",
        "            \"production_budget\",\n",
        "            \"domestic_release_date\",\n",
        "            \"running_time\",\n",
        "            \"keywords\",\n",
        "            \"source\",\n",
        "            \"genre\",\n",
        "            \"production_method\",\n",
        "            \"creative_type\",\n",
        "            \"production_financing_companies\",\n",
        "            \"production_countries\",\n",
        "            \"languages\",\n",
        "        ]:\n",
        "            result[key] = \"\"\n",
        "\n",
        "    # Extract Lead Ensemble Members and Production/Technical Credits\n",
        "    # For the mobile layout, assume these appear in a container with id \"cast-and-crew_mobile\".\n",
        "    cast_mobile = soup.find(\"div\", id=\"cast-and-crew_mobile\")\n",
        "    lead_members = []\n",
        "    prod_tech_credits = []\n",
        "    role_dict = {}\n",
        "\n",
        "    if cast_mobile:\n",
        "        # Extract lead ensemble members.\n",
        "\n",
        "        lead_header = cast_mobile.find(\"h1\", string=lambda s: s and \"Leading Cast\" in s)\n",
        "        if lead_header:\n",
        "            lead_table = lead_header.find_next(\"table\")\n",
        "            if lead_table:\n",
        "                for row in lead_table.find_all(\"tr\"):\n",
        "                    cells = row.find_all(\"td\")\n",
        "                    if len(cells) >= 3:\n",
        "                        actor = cells[0].get_text(strip=True)\n",
        "                        role = cells[2].get_text(strip=True)\n",
        "                        lead_members.append({\"actor\": actor, \"role\": role})\n",
        "        # Extract production and technical credits.\n",
        "        prodtech_header = cast_mobile.find(\n",
        "            \"h1\", string=lambda s: s and \"Production and Technical Credits\" in s\n",
        "        )\n",
        "\n",
        "        if prodtech_header:\n",
        "            credits_table = prodtech_header.find_next(\"table\")\n",
        "            if credits_table:\n",
        "                for row in credits_table.find_all(\"tr\"):\n",
        "                    # Stop processing if a horizontal rule (<hr>) is found in this row.\n",
        "                    if row.find(\"hr\"):\n",
        "                        break\n",
        "                    cells = row.find_all(\"td\")\n",
        "                    if cells:\n",
        "                        # Join all cell texts with \" | \" as separator.\n",
        "                        row_text = \" | \".join(\n",
        "                            cell.get_text(strip=True) for cell in cells\n",
        "                        )\n",
        "                        prod_tech_credits.append(row_text)\n",
        "\n",
        "                credit_list = prod_tech_credits[0].split(\" | \")\n",
        "\n",
        "                # Initialize an empty dictionary to store roles and associated people\n",
        "\n",
        "                # Loop through the list to pair people with their roles\n",
        "                for i in range(0, len(credit_list), 3):\n",
        "                    person = credit_list[i].strip()\n",
        "                    role = (\n",
        "                        credit_list[i + 2].strip() if i + 2 < len(credit_list) else \"\"\n",
        "                    )\n",
        "\n",
        "                    # Add the person to the list for the role in the dictionary\n",
        "                    if role not in role_dict:\n",
        "                        role_dict[role] = []\n",
        "                    role_dict[role].append(person)\n",
        "    result[\"lead_ensemble_members\"] = lead_members\n",
        "\n",
        "    result[\"production_technical_credits\"] = role_dict\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up and Executing the Scraping Process\n",
        "\n",
        "Now we choose the year range (2000-2025) and define a concurrency setup with 100 workers. We create functions to:\n",
        "1. Scrape entire years of movie data\n",
        "2. Extract detailed information for individual movies\n",
        "\n",
        "By using ThreadPoolExecutor, we parallelize the scraping of individual movie details while processing years sequentially. This approach balances efficiency with server load considerations.\n",
        "\n",
        "After collecting all the movie data, we organize everything into a pandas DataFrame for analysis and export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVj6KOcCPjXG",
        "outputId": "a6ca50b4-53f4-4010-f26f-2842a7fad4d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:19<00:00,  5.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2018\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  6.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:22<00:00,  4.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 165.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2021\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 62/62 [00:07<00:00,  8.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 59/59 [00:01<00:00, 47.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:14<00:00,  7.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping year 2025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "start_year = 2000\n",
        "end_year = 2025\n",
        "num_workers = 100\n",
        "base_url = \"https://www.the-numbers.com\"\n",
        "all_movies_data = []\n",
        "\n",
        "\n",
        "# Function to scrape a single year's movie data\n",
        "def scrape_year(year):\n",
        "    print(f\"Scraping year {year}\")\n",
        "    url = f\"https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/released-in-{year}\"\n",
        "\n",
        "    # Extract movie URLs for the year\n",
        "    soup = extract_movies(url)  # Extracts the movie URLs for the current year\n",
        "    movies = extract_movies_with_links(\n",
        "        soup\n",
        "    )  # Extracts the movie URLs for the current year\n",
        "\n",
        "    year_movie_data = []\n",
        "\n",
        "    # Function to scrape individual movie details\n",
        "    def scrape_movie_details(movie_title, movie_url):\n",
        "        full_url = f\"{base_url}{movie_url}\"\n",
        "        movie_info = extract_movie_info_mobile(extract_movies(full_url))\n",
        "        movie_info[\"movie_url\"] = full_url  # Include the movie URL in the data\n",
        "        movie_info[\"movie_title\"] = movie_title  # Include the movie URL in the data\n",
        "        return movie_info\n",
        "\n",
        "    # Use ThreadPoolExecutor to parallelize the scraping of individual movie details\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Submit tasks to scrape each movie concurrently\n",
        "        futures = [\n",
        "            executor.submit(scrape_movie_details, movie_title, movie_url)\n",
        "            for movie_title, movie_url in tqdm(movies.items())\n",
        "        ]\n",
        "\n",
        "        # Wait for all tasks to complete and collect the results\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            year_movie_data.append(future.result())\n",
        "\n",
        "    return year_movie_data\n",
        "\n",
        "\n",
        "# Sequential loop over years, parallelizing the movie detail scraping\n",
        "for year in range(start_year, end_year + 1):\n",
        "    # Scrape the movies for the current year\n",
        "    year_movie_data = scrape_year(year)\n",
        "\n",
        "    # Add the year data to the all_movies_data list\n",
        "    all_movies_data.extend(year_movie_data)\n",
        "\n",
        "# Create a pandas DataFrame from the list of movie data\n",
        "df = pd.DataFrame(all_movies_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examining Our Collected Dataset\n",
        "\n",
        "This cell displays data type and null-count information for the generated DataFrame. We successfully scraped data for 4,625 movies released between 2000 and 2025, creating a comprehensive dataset with details about each film's synopsis, budget, cast, production team, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5g_MPwqpUWj",
        "outputId": "6ff53058-9894-4ed5-b244-2df8fa766805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4625 entries, 0 to 4624\n",
            "Data columns (total 18 columns):\n",
            " #   Column                          Non-Null Count  Dtype \n",
            "---  ------                          --------------  ----- \n",
            " 0   synopsis                        3623 non-null   object\n",
            " 1   opening_weekend                 4329 non-null   object\n",
            " 2   percent_of_total_gross          4329 non-null   object\n",
            " 3   production_budget               4134 non-null   object\n",
            " 4   domestic_release_date           4402 non-null   object\n",
            " 5   running_time                    4545 non-null   object\n",
            " 6   keywords                        4546 non-null   object\n",
            " 7   source                          4614 non-null   object\n",
            " 8   genre                           4621 non-null   object\n",
            " 9   production_method               4621 non-null   object\n",
            " 10  creative_type                   4618 non-null   object\n",
            " 11  production_financing_companies  4062 non-null   object\n",
            " 12  production_countries            4603 non-null   object\n",
            " 13  languages                       4560 non-null   object\n",
            " 14  lead_ensemble_members           4625 non-null   object\n",
            " 15  production_technical_credits    4625 non-null   object\n",
            " 16  movie_url                       4625 non-null   object\n",
            " 17  movie_title                     4625 non-null   object\n",
            "dtypes: object(18)\n",
            "memory usage: 650.5+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Our Dataset\n",
        "\n",
        "We export our comprehensive movie dataset to a CSV file for future analysis or as input to other data processing pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1YnK8ZhpWb3"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"final_df.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
